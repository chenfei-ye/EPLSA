{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a841b2a2",
   "metadata": {},
   "source": [
    "# Operations on fMRI-preprocessing results\n",
    "\n",
    "- Calculate the input required for LEiDA-net, ELA, and EPLSA\n",
    "\n",
    "- Conduct brain dynamic analysis using different methods and calculate brain dynamic indicators\n",
    "\n",
    "- Comparison between groups\n",
    "\n",
    "- The subsequent application to the natural sleep dataset and the OASIS3 dataset is similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ce3739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind,levene,ks_2samp\n",
    "from scipy.stats import permutation_test\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import math\n",
    "from scipy.stats import pearsonr\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da437c59",
   "metadata": {},
   "source": [
    "- Define the required functions in advance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a370967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##计算eigenvector\n",
    "from scipy.signal import hilbert\n",
    "from scipy.sparse.linalg import eigs\n",
    "from nilearn.signal import clean\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Signals utils\n",
    "def fourier_frec(signal_ts,T):\n",
    "    sr = 1./T\n",
    "    if signal_ts.ndim == 1:\n",
    "        signal_f = np.abs(np.fft.fft(signal_ts))\n",
    "        f = np.arange(0, sr / 2, 1. * sr / signal_ts.size)\n",
    "        return f, signal_f[:f.size]\n",
    "    elif signal_ts.ndim == 2:\n",
    "        signal_f = np.abs(np.fft.fft(signal_ts,axis=1))\n",
    "        f = np.arange(0, sr / 2, 1. * sr / signal_ts.shape[1])\n",
    "        return f, signal_f[:, :f.size]\n",
    "    else:\n",
    "        raise Exception('signal_ts has more than 2 dimensions')\n",
    "\n",
    "def hilbert_phase(signals):\n",
    "\n",
    "    phase = hilbert(signals, axis=1)\n",
    "    N_rois = phase.shape[0]\n",
    "    for roi in range(N_rois): \n",
    "        phase[roi, :] = np.angle(phase[roi, :]).real\n",
    "    phase = phase.real\n",
    "    return phase\n",
    "\n",
    "def ang_shortest_diff(a,b):\n",
    "    if np.abs(a-b)> np.pi:\n",
    "        c = 2*np.pi-np.abs(a-b)\n",
    "    else:\n",
    "        c = np.abs(a-b)\n",
    "    return c\n",
    "\n",
    "def clean_signals(signals,detrend=True,standardize='zscore',filter_type=None,low_pass=None,high_pass=None,TR=None):\n",
    "    if not isinstance(signals,dict):\n",
    "        raise ValueError(\"'signals' must be a dictionary!\")\n",
    "    \n",
    "    cleaned_signals = {}\n",
    "    \n",
    "    for sub in signals.keys():\n",
    "        cleaned_signals[sub] = clean(\n",
    "            signals[sub].T,\n",
    "            detrend=detrend, \n",
    "            standardize=standardize,\n",
    "            filter=False if filter_type is None else filter_type, \n",
    "            low_pass=low_pass, \n",
    "            high_pass=high_pass, \n",
    "            t_r=TR\n",
    "            ).T\n",
    "\n",
    "    return cleaned_signals\n",
    "\n",
    "\n",
    "# Matrix utils\n",
    "def phase_coherence(signals_phases):\n",
    "    if not isinstance(signals_phases,np.ndarray) or (isinstance(signals_phases,np.ndarray) and signals_phases.ndim!=2):\n",
    "        raise Exception(\"'signals_phases' must be a 2D array.\")\n",
    "    \n",
    "    N = signals_phases.shape[0] #number of voxels/parcels\n",
    "    T = signals_phases.shape[1]-2 #number of time points/volumes\n",
    "\n",
    "    dFC = np.zeros((N,N,T)) #matrix to save the phase-coherence between regions n and p at time t.\n",
    "    signals_phases = signals_phases[:,1:-1] #delete the fist and last time point of the time series of each ROI signal.\n",
    "    \n",
    "    for time_point in range(T): #for each time point:\n",
    "        for roi_1 in range(N): #for each region of interest\n",
    "            for roi_2 in range(N): #relate with other region of interest\n",
    "                dFC[roi_1,roi_2,time_point] = np.cos(\n",
    "                    ang_shortest_diff(\n",
    "                        signals_phases[roi_1,time_point],\n",
    "                        signals_phases[roi_2,time_point]\n",
    "                        )\n",
    "                    )\n",
    "    return dFC\n",
    "\n",
    "def get_eigenvectors(dFC,n=1):\n",
    "    if not isinstance(dFC,np.ndarray) or (isinstance(dFC,np.ndarray) and dFC.ndim!=3):\n",
    "        raise Exception(\"'dFC' must be a 3D array!\")\n",
    "    \n",
    "    T, N = dFC.shape[-1], dFC.shape[0] #number of time points and number of regions\n",
    "    \n",
    "    LEi = np.empty((T,n*N))\n",
    "    for t in range(T):\n",
    "        avals, avects = eigs(dFC[:,:,t], n, which='LM')\n",
    "        ponderation = avals.real / np.sum(avals.real)\n",
    "        for x in range(avects.shape[1]):\n",
    "            # convention, negative orientation\n",
    "            if np.mean(avects[:, x] > 0) > .5:\n",
    "                avects[:, x] *= -1\n",
    "            elif np.mean(avects[:, x] > 0) == .5 and np.sum(avects[avects[:, x] > 0, x]) > -1. * sum(avects[avects[:, x] < 0, x]):\n",
    "                avects[:, x] *= -1\n",
    "\n",
    "        LEi[t] = np.hstack([p * avects.real[:, x].real for x, p in enumerate(ponderation)])\n",
    "\n",
    "    return LEi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4447f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tseries(path):\n",
    "    try:\n",
    "        signals_path = f'{path}/time_series'\n",
    "        sub_folders = [f for f in os.listdir(signals_path) if os.path.isdir(f'{signals_path}/{f}')]\n",
    "        sub_folders.sort()\n",
    "        tseries = {}\n",
    "        for sub in sub_folders:\n",
    "            for file in os.listdir(f'{signals_path}/{sub}'):\n",
    "                if file.endswith('.csv'):\n",
    "                    tseries[sub] = np.array(\n",
    "                                        pd.read_csv(\n",
    "                                            f'{signals_path}/{sub}/{file}',\n",
    "                                            sep=',',\n",
    "                                            header=None\n",
    "                                            )\n",
    "                                        )\n",
    "                    tseries[sub] = tseries[sub][1:, :]\n",
    "            \n",
    "        return tseries\n",
    "    except:\n",
    "        raise Exception(\"The time series couldn't be loaded.\")\n",
    "def load_classes(data_path):\n",
    "    try:\n",
    "        try:\n",
    "            classes = load_dictionary(f'{data_path}/metadata.pkl')\n",
    "        except:\n",
    "            metadata = pd.read_csv(f'{data_path}/metadata.csv',sep=',')\n",
    "            cols = ['subject_id','condition']\n",
    "            if not all(item in cols for item in list(metadata.columns)):\n",
    "                raise Exception(\"f'{cols} columns must be present in 'metadata.csv'!\")\n",
    "            classes = {}\n",
    "            for sub in np.unique(metadata.subject_id):\n",
    "                classes[sub] = [label for label in metadata[metadata.subject_id == sub].condition]\n",
    "\n",
    "        return classes\n",
    "    except:\n",
    "        raise Exception(\"The groups/conditions labels coudn't be loaded.\")\n",
    "def txt2list(path):\n",
    "    \"\"\"\n",
    "    Load a .txt file as a list.\n",
    "    Note: the .txt file must contain\n",
    "    an entry/value per line/row.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    path : str.\n",
    "        Full path to the .txt file of\n",
    "        interest. E.g.: 'data/rois_labels.txt'\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list_ : list.\n",
    "    \"\"\"\n",
    "    with open(path,'r') as file:\n",
    "        list_ = [line.strip() for line in file]\n",
    "    return list_\n",
    "\n",
    "def load_rois_labels(path):\n",
    "    try:\n",
    "        return txt2list(f'{path}/rois_labels.txt')\n",
    "    except: \n",
    "        raise Exception(\"The ROIs' labels couldn't be loaded \"\n",
    "                        \"from the provided path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fc1e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "import pickle\n",
    "def dwell_times(labels, TR=None, plot=False):\n",
    "    \n",
    "    dwell = {}\n",
    "    for cluster in np.unique(labels):  # for each cluster (aka brain state)\n",
    "        dwell[cluster] = []\n",
    "        dwell[cluster].append([len(list(g[1])) for g in groupby(labels) if g[0]==cluster])  # get a sequence with the times it appears across time.\n",
    "    \n",
    "    # transforming list to array\n",
    "    for cluster in dwell.keys(): \n",
    "        dwell[cluster] = np.concatenate([i for i in dwell[cluster]])\n",
    "    \n",
    "    # compute the average lifetime (TRs or seconds) of each state    \n",
    "    if TR is None:\n",
    "        mean_dwell = pd.DataFrame({'Cluster':[i for i in dwell.keys()],'Mean_lifetime':[np.mean(dwell[i]) for i in dwell.keys()]})\n",
    "    else:\n",
    "        mean_dwell = pd.DataFrame({'Cluster':[i for i in dwell.keys()],'Mean_lifetime':[np.mean(dwell[i])*TR for i in dwell.keys()]})\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.grid()\n",
    "        sns.barplot(x=[f'State {i+1}' for i in range(len(mean_dwell))], y=mean_dwell.Mean_lifetime)\n",
    "        if TR is None:\n",
    "            plt.ylabel('Average lifetime (TRs)', fontsize=15, fontweight='regular') \n",
    "        else:\n",
    "            plt.ylabel('Average lifetime (s)', fontsize=15, fontweight='regular')\n",
    "        plt.xlabel('State', fontsize=15, fontweight='regular') \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "                                            \n",
    "    return dwell, mean_dwell\n",
    "\n",
    "\n",
    "def dwell_times_group(metadata,labels,TR=None,save_results=False,path=None):\n",
    "    if save_results and path is None: \n",
    "        raise Exception('You must provide a path to save the results')\n",
    "    assert metadata.shape[0] == labels.shape[0], \\\n",
    "        \"The number of rows in 'metadata' must be the same as in 'labels'\"\n",
    "    \n",
    "    results = []\n",
    "    #N_clusters = len(np.unique(labels))\n",
    "    for cond in np.unique(metadata.condition):\n",
    "        subs = metadata[metadata.condition==cond].subject_id.values\n",
    "        for s in np.unique(subs):\n",
    "            idx = np.logical_and(metadata.subject_id==s,metadata.condition==cond)\n",
    "            dw = dwell_times(labels[idx],TR=TR,plot=False)[-1]\n",
    "            dw_ = {\n",
    "                **{'subject_id':s,'condition':cond},\n",
    "                **{f'PL_state_{k}':v for k,v in zip(dw.Cluster,dw.Mean_lifetime)}\n",
    "                }\n",
    "            results.append(dw_)\n",
    "    \n",
    "    results = pd.DataFrame(results).fillna(0.0)\n",
    "\n",
    "    #reorder columns by PL_state (necessary because, when a subject do not traverse\n",
    "    # a particular state, that state will be located at the end of the dataframe columns)\n",
    "    N_states = len(results.columns)-2\n",
    "    states_columns = [f'PL_state_{i+1}' for i in range(N_states)]\n",
    "    results = results[['subject_id','condition']+states_columns]\n",
    "\n",
    "    if save_results:\n",
    "        try: \n",
    "            results.to_csv(f'{save_results}/dwell_times.csv',sep='\\t',index=False)\n",
    "        except:\n",
    "            print(\"Warning: 'dwell_times.csv' was not saved in local folder.\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0be33e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_probabilities(labels,k,norm=True,plot=False):\n",
    "    \"\"\"\n",
    "    Compute the transitions between patterns across time\n",
    "    for a single subject. The number of states ('k')\n",
    "    defines the matrix shape, ensuring that, if a given\n",
    "    subject don't traverse a state of the detected states\n",
    "    for the whole group, the matrix will be constructed\n",
    "    correctly, respecting the original number of states.\n",
    "    If a subject don't traverse a particular state,\n",
    "    then the corresponding row will contain all zeros.\n",
    "    \n",
    "    Params:\n",
    "    ------\n",
    "    labels : ndarray with shape (N_time_points,). \n",
    "        Contains the KMeans predicted label\n",
    "        for each time point.\n",
    "\n",
    "    norm : bool. \n",
    "        Whether to normalize the values of the\n",
    "        transitions matrix.\n",
    "\n",
    "    plot : bool. \n",
    "        Whether to plot the transitions matrix.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    transitions : ndarray with shape (N_states,N_states).\n",
    "    \"\"\"\n",
    "\n",
    "    N_states = k\n",
    "    transitions = np.zeros((N_states, N_states)) #create empty matrix \n",
    "    N_volumes = labels.size \n",
    "\n",
    "    #compute the transitions\n",
    "    for volume_idx in range(N_volumes - 1):\n",
    "        _from = labels[volume_idx]\n",
    "        _to = _from + labels[volume_idx+1] - labels[volume_idx]\n",
    "        transitions[_from-1,_to-1]+=1\n",
    "\n",
    "    if norm:\n",
    "        np.seterr(invalid='ignore') # hide potential warning when dividing 0/0 = NaN\n",
    "        transitions = np.divide(\n",
    "            transitions.astype(np.float_),\n",
    "            np.sum(transitions,axis=1).reshape(transitions.shape[1],1)\n",
    "            ) #normalize the values to probabilities\n",
    "\n",
    "    #replace nan (if any) with 0. Can happen when a subject\n",
    "    # doesn't traverse a state at all.\n",
    "    transitions = np.nan_to_num(transitions,copy=True)\n",
    "        \n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        sns.heatmap(\n",
    "            transitions,\n",
    "            annot=True,\n",
    "            cmap='viridis', \n",
    "            square=True,\n",
    "            linecolor='black',\n",
    "            linewidths=0.5,\n",
    "            xticklabels=[f'State {i+1}' for i in range(N_states)],\n",
    "            yticklabels=[f'State {i+1}' for i in range(N_states)],\n",
    "            cbar_kws={\"shrink\": 0.5}\n",
    "            )\n",
    "        plt.yticks(rotation='horizontal')\n",
    "        plt.xlabel('To',fontsize=15,fontweight='regular')\n",
    "        plt.ylabel('From',fontsize=15,fontweight='regular')\n",
    "        plt.tight_layout()\n",
    "\n",
    "    return transitions\n",
    "\n",
    "def transition_probabilities_group(metadata,labels,save_results=False,path=None):\n",
    "    assert metadata.shape[0] == labels.shape[0], \\\n",
    "        \"The number of rows in 'metadata' must be the same as in 'labels'\"\n",
    "    if save_results and path is None: \n",
    "        raise Exception('You must provide a path to save the results')\n",
    "\n",
    "    results = []\n",
    "    subjects_list,cond_list = [],[]\n",
    "    N_clusters = len(np.unique(labels))\n",
    "    for cond in np.unique(metadata.condition):\n",
    "        subs_ids = metadata[metadata.condition==cond].subject_id.values #get an array with the subjects ids that belongs to a given condition.\n",
    "        for subject in np.unique(subs_ids):\n",
    "            idx = np.logical_and(metadata.subject_id==subject,metadata.condition==cond)\n",
    "            tr = transition_probabilities(labels[idx],k=N_clusters,norm=True,plot=False)\n",
    "            results.append(np.ravel(tr)) #vectorize the transitions matrix.\n",
    "            subjects_list.append(subject)\n",
    "            cond_list.append(cond)\n",
    "\n",
    "    results = pd.DataFrame(np.vstack(results))\n",
    "    columns_names = []\n",
    "    for i in range(N_clusters):\n",
    "        for j in range(N_clusters):\n",
    "            columns_names.append(f'From_{i+1}_to_{j+1}')\n",
    "    \n",
    "    results.columns = columns_names\n",
    "    results.insert(0,'subject_id',subjects_list)\n",
    "    results.insert(1,'condition',cond_list)\n",
    "\n",
    "    if save_results: \n",
    "        try:\n",
    "            results.to_csv(f'{path}/transitions_probabilities.csv',sep='\\t',index=False)\n",
    "        except:\n",
    "            print(\"The results could't be saved in .csv file. Please check the provided path.\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e49cbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hedges_g(x1,x2,paired=False):\n",
    "    #sample sizes\n",
    "    n1 = x1.size\n",
    "    n2 = x2.size\n",
    "    \n",
    "    #degrees of freedom\n",
    "    dof = n1+n2-2\n",
    "    \n",
    "    #variances\n",
    "    var1 = np.var(x1)\n",
    "    var2 = np.var(x2)\n",
    "    \n",
    "    #difference in means\n",
    "    m1 = np.mean(x1)\n",
    "    m2 = np.mean(x2)\n",
    "    diff_mean = np.abs(m1-m2)\n",
    "    \n",
    "    #pooled standard deviation\n",
    "    #s1 = np.std(x1)\n",
    "    #s2 = np.std(x2)\n",
    "    \n",
    "    #Hedges's g\n",
    "    if not paired:\n",
    "        s_pooled = np.sqrt(\n",
    "            (((n1-1)*var1)+((n2-1)*var2))/dof\n",
    "            )\n",
    "        g = diff_mean/s_pooled\n",
    "\n",
    "    else:\n",
    "        g = diff_mean / np.sqrt((var1+var2) / 2)\n",
    "\n",
    "    return g\n",
    "\n",
    "def permtest_ind(data,class_column=None,n_perm=5_000,alternative='two-sided'):\n",
    "    #Validation of input data\n",
    "    if class_column is None:\n",
    "        raise ValueError(\"You must specify the 'class_column'.\")\n",
    "    elif class_column not in data.columns:\n",
    "        raise ValueError(f\"The 'class_column' '{class_column}' not founded in 'data'!\")\n",
    "    if not isinstance(n_perm,int):\n",
    "        raise TypeError(\"'n_perm' must be an integer!\")\n",
    "    \n",
    "    features = [col for col in data if col!=class_column] #list with variables to be tested\n",
    "    n_tests = len(features) #number of tests that will be executed\n",
    "    groups = np.unique(data[class_column]) #get the groups names.\n",
    "    results = [] #list to save results\n",
    "    p_values = []\n",
    "    \n",
    "    for col in features:\n",
    "        x1 = data[data[class_column]==groups[0]][col].values #data of first group.\n",
    "        x2 = data[data[class_column]==groups[1]][col].values #data of the other group.\n",
    "\n",
    "        #running Levene's test\n",
    "        _,p_levene = levene(x1,x2,center='mean')\n",
    "\n",
    "        #running the permutation test\n",
    "        test = ttest_ind(\n",
    "            x1,\n",
    "            x2,\n",
    "            alternative=alternative,\n",
    "            permutations=n_perm,\n",
    "            equal_var=True if p_levene>0.05 else False\n",
    "        )\n",
    "#         p_values.append = [test.pvalue]\n",
    "        p_values.append(test.pvalue)\n",
    "#         _, p_adjusted, _, _ = multipletests(p_values, method='fdr_bh')\n",
    "        #computing effect size\n",
    "        eff = hedges_g(x1,x2)\n",
    "\n",
    "        results.append({\n",
    "            #'k': k,\n",
    "            'variable':col, \n",
    "            'group_1':groups[0],\n",
    "            'group_2':groups[-1],\n",
    "            'statistic':test.statistic,\n",
    "            'p-value':test.pvalue,\n",
    "            'test':'t-test' if p_levene>0.05 else 'welch',\n",
    "            'effect_size':eff,\n",
    "#             'p-value_FDR':p_adjusted,\n",
    "#             'reject_null_FDR':True if p_adjusted < 0.05 else False\n",
    "            })\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    _, p_adjusted, _, _ = multipletests(p_values, method='fdr_bh')\n",
    "    results['p-value_FDR'] = p_adjusted\n",
    "    results['reject_null_FDR'] = [True if p < 0.05 else False for p in p_adjusted]\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def _statistic(x, y, axis):\n",
    "    return np.mean(x, axis=axis) - np.mean(y, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612cf75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dynamics(basin_number, nmin):\n",
    "    # Get Dynamics\n",
    "    num = len(basin_number)\n",
    "    \n",
    "    # Direct transition between basin\n",
    "    dtrans = np.zeros((nmin, nmin))\n",
    "    \n",
    "    for i in range(nmin):\n",
    "        for j in range(nmin):\n",
    "            if i == j:\n",
    "                continue\n",
    "            cnt = 0\n",
    "            for t in range(num - 1):\n",
    "                if basin_number[t] == i+1 and basin_number[t + 1] == j+1:\n",
    "                    cnt = cnt + 1\n",
    "#                 cnt = np.sum((basin_number[:-1] == i) & (basin_number[1:] == j))\n",
    "            dtrans[i, j] = cnt / num\n",
    "    \n",
    "    # Direct and indirect transition between basin\n",
    "    trans = np.zeros((nmin, nmin))\n",
    "\n",
    "    for i in range(nmin):\n",
    "        for j in range(nmin):\n",
    "            if i == j:\n",
    "                continue\n",
    "            ind_i = basin_number == i+1\n",
    "            ind_j = basin_number == j+1\n",
    "            cnt = 0\n",
    "\n",
    "            st_i = np.where(ind_i)[0]\n",
    "            if len(st_i) == 0:\n",
    "                continue\n",
    "            if st_i[0] >= num:\n",
    "                continue\n",
    "\n",
    "            st_j = np.where(ind_j)[0]\n",
    "            if len(st_j) == 0:\n",
    "                continue\n",
    "            if st_j[0] >= num:\n",
    "                continue\n",
    "\n",
    "            st_ii = st_i[0]\n",
    "            while True:\n",
    "                tmp_j = np.where(st_j > st_ii)[0]\n",
    "                if len(tmp_j) == 0:\n",
    "                    break\n",
    "            \n",
    "                cnt += 1\n",
    "            \n",
    "                st_jj = st_j[tmp_j[0]]\n",
    "                tmp_i = np.where(st_i > st_jj)[0]\n",
    "                if len(tmp_i) == 0:\n",
    "                    break\n",
    "                st_ii = st_i[tmp_i[0]]\n",
    "\n",
    "            trans[i, j] = cnt / num\n",
    "    \n",
    "    # Set return value\n",
    "    dynamics = {\n",
    "        'Dtrans': dtrans,\n",
    "        'Trans': trans\n",
    "    }\n",
    "    \n",
    "    return dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14e7fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "##FS\n",
    "def calculate_nodal_functional_segregation_strength(ts, modu1, modu2, num_parcel):\n",
    "    node_fs = [None] * num_parcel\n",
    "\n",
    "    for i in range(num_parcel):\n",
    "        fc_in = []\n",
    "        fc_ac = []\n",
    "        if i in modu1:\n",
    "            for j in modu1:\n",
    "                if j != i:\n",
    "                    corr_coeff, p_value = pearsonr(ts[:, i], ts[:, j])\n",
    "                    fc_in.append(0.5 * math.log((1 + corr_coeff) / (1 - corr_coeff)))\n",
    "            for j in modu2:\n",
    "                corr_coeff, p_value = pearsonr(ts[:, i], ts[:, j])\n",
    "                fc_ac.append(0.5 * math.log((1 + corr_coeff) / (1 - corr_coeff)))\n",
    "        if i in modu2:\n",
    "            for j in modu1:\n",
    "                corr_coeff, p_value = pearsonr(ts[:, i], ts[:, j])\n",
    "                fc_ac.append(0.5 * math.log((1 + corr_coeff) / (1 - corr_coeff)))\n",
    "            for j in modu2:\n",
    "                if j != i:\n",
    "                    corr_coeff, p_value = pearsonr(ts[:, i], ts[:, j])\n",
    "                    fc_in.append(0.5 * math.log((1 + corr_coeff) / (1 - corr_coeff)))\n",
    "\n",
    "        node_fs[i] = np.mean(fc_in) - np.mean(fc_ac)\n",
    "\n",
    "    return node_fs\n",
    "\n",
    "def calculate_functional_segregation_strength(ts, modu1, modu2):\n",
    "    fc1 = []\n",
    "    fc2 = []\n",
    "    fc12 = []\n",
    "    for i in range(len(modu1)):\n",
    "        for j in range(i + 1, len(modu1)):\n",
    "            corr_coeff, p_value = pearsonr(ts[:, modu1[i]], ts[:, modu1[j]])\n",
    "            fc1.append(0.5 * math.log((1 + corr_coeff) / (1 - corr_coeff)))\n",
    "    for i in range(len(modu2)):\n",
    "        for j in range(i + 1, len(modu2)):\n",
    "            corr_coeff, p_value = pearsonr(ts[:, modu2[i]], ts[:, modu2[j]])\n",
    "            fc2.append(0.5 * math.log((1 + corr_coeff) / (1 - corr_coeff)))\n",
    "    for i in range(len(modu1)):\n",
    "        for j in range(len(modu2)):\n",
    "            corr_coeff, p_value = pearsonr(ts[:, modu1[i]], ts[:, modu2[j]])\n",
    "            fc12.append(0.5 * math.log((1 + corr_coeff) / (1 - corr_coeff)))\n",
    "\n",
    "    within_module_fc = fc1 + fc2\n",
    "    across_module_fc = fc12\n",
    "\n",
    "    # Calculate the average within-module FC\n",
    "    average_within_module_fc = np.mean(within_module_fc)\n",
    "\n",
    "    # Calculate the average across-module FC\n",
    "    average_across_module_fc = np.mean(across_module_fc)\n",
    "\n",
    "    # Calculate the functional segregation strength as the difference between the averages\n",
    "    functional_segregation_strength = average_within_module_fc - average_across_module_fc\n",
    "\n",
    "    return average_within_module_fc, average_across_module_fc, functional_segregation_strength\n",
    "\n",
    "\n",
    "def load_dictionary(filepath):\n",
    "    with open(filepath, 'rb') as file:\n",
    "        dict_ = pickle.load(file)\n",
    "        return dict_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0214dc",
   "metadata": {},
   "source": [
    "- Based on the Schaefer400 information of the brain atlas, the preprocessed time series is processed according to the network to obtain the network-based time series as the input of the LEiDA-net method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1238e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "icn_file = '/data/dzy/bids_INV2/derivatives/fig5-lesion_seed/schaefer400x7_MNI.csv' \n",
    "icn_data = pd.read_csv(icn_file)\n",
    "folder = f'/data/dzy/combined_method/AD/normal_amyloid_group/'\n",
    "# for i in range(1,185):\n",
    "i=80\n",
    "for filename in sorted(os.listdir(folder)):\n",
    "    folder_path = os.path.join(folder, filename)\n",
    "    print(folder_path)\n",
    "    i+=1\n",
    "    folder_path_net = f'/data/dzy/combined_method/AD/leida_net_amyloid_group/time_series/sub-{i:04d}'\n",
    "    folder_path_roi = f'/data/dzy/combined_method/AD/leida_roi_amyloid_group/time_series/sub-{i:04d}'\n",
    "    if not os.path.exists(folder_path_net):\n",
    "        os.makedirs(folder_path_net)\n",
    "    if not os.path.exists(folder_path_roi):\n",
    "        os.makedirs(folder_path_roi)\n",
    "    bold_data = pd.read_csv(folder_path)\n",
    "    bold_data = bold_data.T\n",
    "    bold_data.to_csv(f'/data/dzy/combined_method/AD/leida_roi_amyloid_group/time_series/sub-{i:04d}/sub-{i:04d}.csv',index = False)\n",
    "    if 'Name' in icn_data.columns:\n",
    "        bold_data.index = icn_data['Name']\n",
    "    bold_data = bold_data.T\n",
    "    new_data = pd.DataFrame()\n",
    "    for icn in icn_data['ICN'].unique():\n",
    "        icn_col = icn_data[icn_data['ICN']==icn]['Name'].tolist()\n",
    "        icn_group = bold_data[icn_col]\n",
    "        new_data[icn] = icn_group.mean(axis=1)\n",
    "    new_data = new_data.T\n",
    "    df = pd.DataFrame(new_data)\n",
    "    row_means = df.mean(axis=1)\n",
    "    sign_data = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "    for col in df.columns:\n",
    "        for idx in df.index:\n",
    "            value = df.at[idx, col]\n",
    "            if value > row_means[idx]:\n",
    "                sign_data.at[idx, col] = 1\n",
    "            elif value < row_means[idx]:\n",
    "                sign_data.at[idx, col] = -1\n",
    "    new_data.to_csv(f'/data/dzy/combined_method/AD/leida_net_amyloid_group/time_series/sub-{i:04d}/sub-{i:04d}.csv',index = False,sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ec04c3",
   "metadata": {},
   "source": [
    "- Based on the results of LEiDA-net obtained above, calculate the principal eigenvector and binarize it, which is the input of the EPLSA method. Reference [energy landscape analysis toolbox](https://github.com/tkEzaki/energy-landscape-analysis) classify the brain state and some index calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea617bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/data/dzy/combined_method/AD/leida_net_amyloid_group'\n",
    "subject_ids = list(load_tseries(data_path).keys())  # list of subject ids\n",
    "N_subjects = len(subject_ids)  # number of provided subjects\n",
    "eigens = []\n",
    "sub_list, class_list = [], []\n",
    "\n",
    "# Starting process\n",
    "print(\"\\n-STARTING THE PROCESS:\\n\"\n",
    "         \"========================\\n\"\n",
    "        f\"-Number of subjects: {N_subjects}\")\n",
    "\n",
    "print(\"\\n 1) EXTRACTING THE EIGENVECTORS.\\n\")\n",
    "time_series = load_tseries(data_path)\n",
    "time_series = clean_signals(\n",
    "    signals=time_series,\n",
    "    detrend=True, \n",
    "    standardize='zscore',  # z-score\n",
    "    filter_type='butterworth',  #Butterworth\n",
    "    low_pass=0.1,  \n",
    "    high_pass=0.01,  \n",
    "    TR=2.5  \n",
    ")\n",
    "classes = load_classes(data_path)\n",
    "rois_labels = load_rois_labels(data_path)\n",
    "for sub_idx, sub_id in enumerate(subject_ids):  # for each subject\n",
    "            # get current subject signals\n",
    "\n",
    "    tseries = time_series[sub_id]\n",
    "    N_volumes = tseries.shape[1] - 2\n",
    "\n",
    "    print(f\"SUBJECT ID: {sub_id} ({tseries.shape[1]} volumes)\")\n",
    "\n",
    "    # Extract the eigenvectors from each phase-coherence matrix at time t.\n",
    "    eigens.append(\n",
    "        get_eigenvectors(phase_coherence(hilbert_phase(tseries)))\n",
    "    )\n",
    "\n",
    "    # Append metadata to lists (to complete the eigenvectors dataset)\n",
    "    for volume in range(N_volumes):\n",
    "        sub_list.append(sub_id)\n",
    "        if len(classes[sub_id]) > 1:\n",
    "            class_list.append(classes[sub_id][volume + 1])\n",
    "        else:\n",
    "            class_list.append(classes[sub_id][0])\n",
    "\n",
    "df_eigens = pd.DataFrame(np.vstack(eigens), columns=rois_labels)\n",
    "df_eigens.insert(0, 'subject_id', sub_list)\n",
    "df_eigens.insert(1, 'condition', class_list)\n",
    "\n",
    "# saving results\n",
    "df_eigens.to_csv(f'/data/dzy/combined_method/AD/EPLSA_amyloid_group/eigenvectors.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac11eb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825\n"
     ]
    }
   ],
   "source": [
    "##Obtain the main feature vector data and binarize it\n",
    "eigenvector_data = pd.read_csv('/data/dzy/combined_method/AD/EPLSA_amyloid_group/eigenvectors.csv',sep='\\t')\n",
    "sub_id = eigenvector_data['subject_id']\n",
    "unique_sub_IDs = np.unique(sub_id)\n",
    "print(len(unique_sub_IDs))\n",
    "for sub_ID in unique_sub_IDs:\n",
    "    sub_data_subset = eigenvector_data[eigenvector_data['subject_id'] == sub_ID]\n",
    "    df = pd.DataFrame(sub_data_subset)\n",
    "    df = df.iloc[:, 2:]\n",
    "    df = df.T\n",
    "    sign_data = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "    for col in df.columns:\n",
    "        for idx in df.index:\n",
    "            value = df.at[idx, col]\n",
    "            if value > 0:\n",
    "                sign_data.at[idx, col] = 1\n",
    "            elif value < 0:\n",
    "                sign_data.at[idx, col] = -1\n",
    "    sign_data.to_csv(f'/data/dzy/combined_method/AD/EPLSA_amyloid_group/dat/{sub_ID}.dat', index=False, header=False, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182fb995",
   "metadata": {},
   "source": [
    "- Calculate the principal eigenvector based on the results of LEiDA-roi and binarize it, which is the input of the ELA method. Reference [energy landscape analysis toolbox](https://github.com/tkEzaki/energy-landscape-analysis) classify the brain state and some index calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09948b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,525):\n",
    "    data_path = f'/data/dzy/combined_method/ASD_new/200_400/time_series/sub-{i:03d}/sub-{i:03d}.csv'\n",
    "    sub_data_subset = pd.read_csv(data_path)\n",
    "    df = pd.DataFrame(sub_data_subset)\n",
    "    row_means = df.mean(axis=1)\n",
    "    sign_data = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "    for col in df.columns:\n",
    "        for idx in df.index:\n",
    "            value = df.at[idx, col]\n",
    "            if value > row_means[idx]:\n",
    "                sign_data.at[idx, col] = 1\n",
    "            elif value < row_means[idx]:\n",
    "                sign_data.at[idx, col] = -1\n",
    "    sign_data.to_csv(f'/data/dzy/combined_method/ASD_new/200_400/ELA/dat/sub-{i:03d}.dat', index=False, header=False, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b3b794",
   "metadata": {},
   "source": [
    "- ELA, EPLSA brain dynamic index method, two LEiDA method of index calculating reference [LEiDA](https://github.com/juanitacabral/LEiDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71bdbe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Calculate the dwell time and transition probability of each subject in each state\n",
    "labels = []\n",
    "for i in range(1,292):\n",
    "    label = pd.read_csv(f'/data/dzy/combined_method/AD/EPLSA_amyloid/result/sub-{i:04d}_BN.csv', header=None)\n",
    "    label.insert(0, 'subject_id', f'sub-{i:04d}')\n",
    "    label.insert(1, 'condition', 'AD')\n",
    "    labels.append(label)\n",
    "for b in range(292,932):\n",
    "    label = pd.read_csv(f'/data/dzy/combined_method/AD/EPLSA_amyloid/result/sub-{b:04d}_BN.csv', header=None)\n",
    "    label.insert(0, 'subject_id', f'sub-{b:04d}')\n",
    "    label.insert(1, 'condition', 'HC')\n",
    "    labels.append(label)\n",
    "\n",
    "labels = pd.concat(labels, ignore_index=True)\n",
    "meta = labels[['subject_id','condition']]\n",
    "ys = labels.iloc[:,2].values\n",
    "\n",
    "dwelltimes = dwell_times_group(\n",
    "            meta,\n",
    "            ys,\n",
    "            TR=2,\n",
    "            save_results=False,\n",
    "            path = '/data/dzy/combined_method/AD/EPLSA_amyloid/result'\n",
    "            )\n",
    "dwelltimes.to_csv('/data/dzy/combined_method/AD/EPLSA_amyloid/result/dwell_time.csv',sep='\\t',index=False)\n",
    "\n",
    "transitions = transition_probabilities_group(\n",
    "            meta,\n",
    "            ys,\n",
    "            save_results=False,\n",
    "            path= '/data/dzy/combined_method/AD/EPLSA_amyloid/result'\n",
    "            )\n",
    "transitions.to_csv('/data/dzy/combined_method/AD/EPLSA_amyloid/result/transition_probabilities.csv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8311d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "dynamics_results = {}\n",
    "basin_numbers = labels.iloc[:, 2].values\n",
    "nmin = 6  \n",
    "for subject_id in labels['subject_id'].unique():\n",
    "    subject_basin_numbers = basin_numbers[labels['subject_id'] == subject_id]\n",
    "    dynamics_results[subject_id] = get_dynamics(subject_basin_numbers, nmin)\n",
    "\n",
    "# for subject_id, dynamics in dynamics_results.items():\n",
    "#     print(f\"Subject ID: {subject_id}\")\n",
    "#     print(\"Direct transition between basin:\\n\", dynamics['Dtrans'])\n",
    "#     print(\"Direct and indirect transition between basin:\\n\", dynamics['Trans'])\n",
    "#     print()\n",
    "    \n",
    "output_file = f'/data/dzy/combined_method/AD/EPLSA_baseline/result_baseall/Transition.csv'  # 构造输出文件名\n",
    "\n",
    "columns = ['InputFile']\n",
    "for i in range(nmin):\n",
    "    for j in range(nmin):\n",
    "        if i != j:\n",
    "            columns.append(f'Direct transitions from B{i+1} to B{j+1}')\n",
    "for i in range(nmin):\n",
    "    for j in range(nmin):\n",
    "        if i != j:\n",
    "            columns.append(f'Transitions from B{i+1} to B{j+1}')\n",
    "\n",
    "data = []\n",
    "for k, (subject_id, dynamics) in enumerate(dynamics_results.items()):\n",
    "    row = [subject_id]\n",
    "    for i in range(nmin):\n",
    "        for j in range(nmin):\n",
    "            if i != j:\n",
    "                row.append(dynamics['Dtrans'][i, j])\n",
    "    for i in range(nmin):\n",
    "        for j in range(nmin):\n",
    "            if i != j:\n",
    "                row.append(dynamics['Trans'][i, j])\n",
    "    data.append(row)\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f147b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/data/dzy/combined_method/AD/leida_net_baseline'\n",
    "time_series = load_tseries(data_path)\n",
    "time_series = clean_signals(\n",
    "    signals=time_series,\n",
    "    detrend=True,  \n",
    "    standardize='zscore',  \n",
    "    filter_type='butterworth', \n",
    "    low_pass=0.1, \n",
    "    high_pass=0.01,  \n",
    "    TR=2.5  \n",
    ")\n",
    "modu1 = pd.read_csv('/data/dzy/combined_method/AD/EMSA/6_modu1.csv')\n",
    "modu1 = modu1['Index'].tolist()\n",
    "# print(modu1)\n",
    "modu2 = pd.read_csv('/data/dzy/combined_method/AD/EMSA/6_modu2.csv')\n",
    "modu2 = modu2['Index'].tolist()\n",
    "num_parcel = 7\n",
    "subject_ids = list(time_series.keys())  # list of subject ids\n",
    "N_subjects = len(subject_ids)  # number of provided subjects\n",
    "node_fs_list = []\n",
    "average_within_module_fc_list = []\n",
    "average_across_module_fc_list = []\n",
    "functional_segregation_strength_list = []\n",
    "for sub_idx, sub_id in enumerate(subject_ids):  # for each subject\n",
    "    # get current subject signals\n",
    "    tseries = time_series[sub_id]\n",
    "#     print(tseries)\n",
    "    tseries = tseries.T\n",
    "    print(tseries)\n",
    "    node_fs = calculate_nodal_functional_segregation_strength(tseries, modu1, modu2, num_parcel)\n",
    "    node_fs_list.append(node_fs)\n",
    "    average_within_module_fc, average_across_module_fc, functional_segregation_strength = calculate_functional_segregation_strength(tseries, modu1, modu2)\n",
    "    average_within_module_fc_list.append(average_within_module_fc)\n",
    "    average_across_module_fc_list.append(average_across_module_fc)\n",
    "    functional_segregation_strength_list.append(functional_segregation_strength)\n",
    "results_df = pd.DataFrame({\n",
    "    'Subject ID': subject_ids,\n",
    "    'Node FS': node_fs_list,\n",
    "    'Average_Within_Module_FC': average_within_module_fc_list,\n",
    "    'Average_Across_Module_FC': average_across_module_fc_list,\n",
    "    'Functional_Segregation_Strength': functional_segregation_strength_list\n",
    "})\n",
    "results_df.to_csv('/data/dzy/combined_method/AD/EPLSA_baseline/result/FS7.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e687c323",
   "metadata": {},
   "source": [
    "- Significance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9612540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "##Significance analysis of indicators\n",
    "data = pd.read_csv('/data/dzy/combined_method/AD/EPLSA_amyloid/result_ad39_hc362/Dynamics_20250622_16_28_08_new.csv',sep =',')\n",
    "data = data.dropna(subset=['dx'])\n",
    "conditions = np.unique(data.dx)\n",
    "N_conditions = conditions.size\n",
    "results_stacked = []\n",
    "for conds in combinations(conditions,2): \n",
    "    data_ = data[data.dx.isin(conds)] \n",
    "    stats_results = permtest_ind(\n",
    "        data_,\n",
    "        class_column='dx',\n",
    "        alternative='two-sided',\n",
    "        n_perm=5_000\n",
    "        )\n",
    "    results_stacked.append(stats_results)\n",
    "if N_conditions>2:\n",
    "        metric_results = pd.concat(results_stacked,axis=0).reset_index(drop=True)\n",
    "else:\n",
    "        metric_results = pd.DataFrame(stats_results)\n",
    "print(metric_results)\n",
    "metric_results.to_csv(f'/data/dzy/combined_method/AD/EPLSA_amyloid/result_ad39_hc362/occ_new.csv',sep='\\t',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
